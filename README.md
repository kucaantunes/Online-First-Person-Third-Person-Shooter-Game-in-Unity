# Online-First-Person-Third-Person-Shooter-Game-in-Unity
An online FPS / TPS game wuth advanced AI

Full source code: https://mega.nz/file/LiBngaDA#T4S7TL3vPMs1-b4IAZXwpdVMbpYA8Zt8WccV2ssWErM


![image](https://github.com/user-attachments/assets/96241e52-4de5-4716-915f-580deba72db0)


![image](https://github.com/user-attachments/assets/a59bbce1-3ecd-4de4-8c22-64023beb995f)

![image](https://github.com/user-attachments/assets/625a7c69-34a0-4df2-ba28-87819b917004)

Vuforia engine was used in order to obtain the AR Camera associated to the Unity Engine which allows the 
use of simultaneous localization and mapping (SLAM) and create occlusions with augmented and real 
image elements. 
Figure 36. Use of Vuforia to correctly place AI agents in the scenario captured by the phone camera.  
With the use of Vuforia AR Camera and ground plane detection it is possible to place correctly the 
computer-generated image in the LCD taking in consideration the 3D physics of the environment captured 
by the phone camera.
![image](https://github.com/user-attachments/assets/f0a8cbdd-b98e-4e81-a8ed-71b40ce505ff)

Via the Unity IDE it is possible to configure the camera settings and the lightning and also to implement 
AI mechanisms like the behavior of the virtual elements among other possible functionalities. The 
programming language used to develop the prototype was C# and some libraries and classes were 
imported in order to implement all the features of a mixed reality first person shooter for mobile phones 
compatible with some of the operating systems of the android family. 
Figure 38. Preview of the prototype ARFPS. 
Concerning the shooting system, a ray cast function was used taking in consideration direction, speed, 
position and distance and also spawner game object mechanisms in order for the bullets to appear after 
the verification of certain conditions like when the user presses the shooting button or when the AI agent or 
enemy is at a certain distance of the main player. 
Figure 39. Placing the camera in the Unity Game Engine 
39 
The animations follow a set of conditions and sequences in order to be activated, the picture below 
shows a schematic of the necessary order and trigger mechanisms associated to the animation transitions 
process during gameplay. 
Figure 40. Making animation transitions in Unity 
The animations follow an AI mechanism in order to provide more functionalities to MR systems as 
demonstrated in this prototype, it is possible to notice that the virtual elements can have more functionalities 
and interaction with the users. In the future will be possible that applications may have more uses from the 
real-world data captured by a sensor, the infrared (IR) cameras for example can provide night vision. 
The process of treating information has been improving and the AI strategies nowadays are able to 
treat huger amounts of data making predictions with a high rate of accuracy. 
The presented developed prototype illustrates a process of responding to the second research question 
by demonstrating a set of processes to achieve the proposed objective, for this demonstration was used 
the C# programming language and the Unity game engine among other technologies.

![image](https://github.com/user-attachments/assets/28905703-f85f-4ab0-bcf9-ca0634725cbf)

Virtual Reality applications have the ability of providing the sensation of immersion, usually the VR 
glasses combine a process of blending two images, one for each eye allowing for the user to see a 3D 
virtual world. On a perspective of accessibility can help for some cognitive, auditory and physical disabilities. 
The VR devices might have a remote or not, without a remote a virtual laser bean can work as a pointer 
generating interaction with parts of the application that are programmed to perform certain functionalities, 
using collisions triggers. The Google VR SDK was used, which allows to provide the VR format for unity. 
As demonstrated in the figure below in order to generate the effect of the immersion and for the application 
to be correctly seen when using this type of VR glasses described, the application has to be divided in two 
screens the first for the left eye and the second for the right eye.  
Figure 47. VR App on the mobile phone before using glasses; baking the AI area, usage of the VR 
glasses, game for training that uses a laser bean based on head movement and when the mission is 
accomplished shows an educational video on how to proceed in case of heart attacks. 
44 
In terms of usability, VR technologies have showed relevant results for training activities, some VR 
applications simulate an operating room, showing doctors how to proceed concerning certain types of 
surgeries. For some functionalities under some contexts VR can help to achieve the objectives in a more 
interactive and efficient way. 
The VR application developed has the objective of providing training where videos explaining how to 
proceed for certain clinical emergencies are played after successfully completing the game, aiming for an 
innovative way of providing formation. 
With the objective of making the applications more interactive and accessible to people with auditory 
and visible disabilities and to improve the performance and functionality of the Information Systems (IS), 
the dissertation shows a strategy of using another sensor instead of a camera, which in this case is the 
leap motion controller that allows to detect and place on the screen the hands of the user. 
There has been an increased use of VR technologies for training activities, the implementation of AI 
mechanisms allow to provide more functionalities to the applications. 
Figure 48. VR Leap Motion Controller. Features highly accurate V4 hand and finger tracking. Offers a 
135Â° field of view and up to 80 cm range. Tracks objects and captures high-speed infrared footage 
Interacts directly with digital content, VR & AR apps. 
A system that uses this type of sensor allows the user to interact with the application via alternate 
processes than the traditional computer and keyboard allowing the possibility of being used for different 
type of users and ages. 
The VR application developed for this sensor allows the simulation of a surgery displaying the details 
of certain devices. 
The purpose is based in simulating a real-world scenario in order to make the formation more appealing 
and interactive taking in consideration the risks of performing certain surgery operations. 
45 
 
 
46 
 
 
 
Figure 49. Developed Healthcare Simulator with leap motion, for training purposes. 
The objective is to facilitate the training of the doctors to perform a surgery, in this case the leap motion 
sensor allows to display the hands on the screen, allowing a different type of interaction making the 
application more appealing because some surgery operations require the use of the hands, which the 
movements can not be trained by using a mouse or typing a keyboard.

Learning activities can be more interactive and accessible for users of different ages and types of 
disabilities. Based on the context of training and to facilitate the performance of physiotherapeutic tasks a 
controller that detects, hands, arms, legs, head and feet was used to answer the third research question of 
this investigation. 
Based on the creation of AI Agents, like the creation of a Computer-generated Imagery (CGI) girl, the 
sensor which in this case is the webcam requires to capture the all body in order to associate geometries 
and bones. A virtual girl replicates the user movements on a projected Liquid Crystal Display (LCD) that 
aims to help in physiotherapeutic terms by providing movements and exercises that can help some motor 
disabilities. The prototype was developed in C# using the Game Engine Unity providing a Mixed Reality 
(MR) experience. On a perspective of virtual storytelling, this project uses a process of providing interactivity 
to the user for story creation. 
Figure 62. Full body motion and detector based on the bones structure. 
The technology used can help people with visual problems to interact with the system with all parts of 
the body having the head, arms, body and legs more relevance concerning the detection process. 
Via C# the developed prototype is able to associate the animations to the values detected by the 
camera. 
62 
Figure 63. Performing test with full body motion controller. 
During tests it was noticed a high level of satisfaction from the users, mainly due to being a new 
technology. The MR system in cause allows to place virtual elements in the images captured by the camera, 
the objective was possible to achieve via the Unity game engine. 
In order for the application to work the camera has to detect all part of the bodies, so users can not be 
close to the sensor and should be at a distance superior to 80 cms to perform better results. 
There are other sensors like the Wii 2 that allow to interact with many parts of the body. 
Figure 64. Wii 2 for full body tracking.


